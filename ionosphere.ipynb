{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Pooja-Shanbhag/Binary-classification-Ionosphere/blob/master/ionosphere.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FIBRCZ_NGrlY"
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.nn import Module\n",
    "from torch.nn import Linear\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import Sigmoid\n",
    "from torch.nn import BCELoss\n",
    "from torch.nn.init import kaiming_uniform_\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from torch.optim import SGD\n",
    "from numpy import vstack\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5KuiGvh1Grlb"
   },
   "source": [
    "### Dataset definition\n",
    "PyTorch provides the Dataset class that you can extend and customize to load your dataset.\n",
    "For example, the constructor of your dataset object can load your data file (e.g. a CSV file). You can then override the __len__() function that can be used to get the length of the dataset (number of rows or samples), and the __getitem__() function that is used to get a specific sample by index.\n",
    "\n",
    "The random_split() function can be used to split a dataset into train and test sets. Here we are dividing 67 train and 33 test data.\n",
    "\n",
    "##### Current CSVDataset\n",
    "label is the last one in the csv. It is a string represented as 'g': good and 'b': bad.\n",
    "We use LabelEncoder to encode string labels to 0 and 1's.\n",
    "We convert X and y's as floats.\n",
    "\n",
    "Why self.y.reshape?\n",
    "Here we convert 1D array of size M to 2D array of size M*1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Wy-dyP1Grlc"
   },
   "outputs": [],
   "source": [
    "class CSVDataset(Dataset):\n",
    "    def __init__(self,path):\n",
    "        dataframe = read_csv(path, header = None)\n",
    "        \n",
    "        self.X = dataframe.values[:,:-1]\n",
    "        self.y = dataframe.values[:,-1]\n",
    "        \n",
    "        self.X = self.X.astype('float32')\n",
    "        self.y = LabelEncoder().fit_transform(self.y)\n",
    "        self.y = self.y.astype('float32')\n",
    "#         print(self.y.shape)\n",
    "#         print(self.y)\n",
    "        self.y = self.y.reshape(len(self.y),1)\n",
    "#         print(self.y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return [self.X[idx],self.y[idx]]\n",
    "    \n",
    "    def get_split_data(self,n_train = 0.67):\n",
    "        train_len = round(len(self.X)*n_train)\n",
    "        test_len = len(self.X) - train_len\n",
    "        return random_split(self,[train_len,test_len])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U2KjPniQGrlf"
   },
   "source": [
    "### Model definition\n",
    "The idiom for defining a model in PyTorch involves defining a class that extends the Module class.\n",
    "\n",
    "The constructor of your class defines the layers of the model and the forward() function is the override that defines how to forward propagate input through the defined layers of the model.\n",
    "\n",
    "Many layers are available, such as Linear for fully connected layers, Conv2d for convolutional layers, and MaxPool2d for pooling layers.\n",
    "\n",
    "Activation functions can also be defined as layers, such as ReLU, Softmax, and Sigmoid.\n",
    "\n",
    "#### Current model: \n",
    "Here we are using 3 hidden layers and relu activation with a 'He Uniform' weight initialization. This combination goes a long way to overcome the problem of vanishing gradients when training deep neural network models.\n",
    "In the last layer we use the sigmoid function.\n",
    "\n",
    "kaiming_uniform_(Tensor tensor, double a = 0, FanModeType mode = torch::kFanIn, NonlinearityType nonlinearity = torch::kLeakyReLU) --> Fills the input Tensor with values using a uniform distribution. Also known as He initialization. No gradient will be recorded for this operation.\n",
    "\n",
    "xavier_uniform_(Tensor tensor, double gain = 1.0) --> Fills the input Tensor with values using a uniform distribution. Values are scaled by the gain parameter No gradient will be recorded for this operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DDqOiPwTGrlg"
   },
   "outputs": [],
   "source": [
    "class MLP(Module):\n",
    "    def __init__(self, inputs):\n",
    "        super(MLP,self).__init__()\n",
    "        \n",
    "        self.hidden1 = Linear(inputs,10)\n",
    "        kaiming_uniform_(self.hidden1.weight,nonlinearity = 'relu')\n",
    "        self.act1 = ReLU()\n",
    "        \n",
    "        self.hidden2 = Linear(10,8)\n",
    "        kaiming_uniform_(self.hidden2.weight, nonlinearity = 'relu')\n",
    "        self.act2 = ReLU()\n",
    "        \n",
    "        self.hidden3 = Linear(8,1)\n",
    "        xavier_uniform_(self.hidden3.weight)\n",
    "        self.act3 = Sigmoid()\n",
    "        \n",
    "    def forward(self,X):\n",
    "        X = self.hidden1(X)\n",
    "        X = self.act1(X)\n",
    "        \n",
    "        X = self.hidden2(X)\n",
    "        X = self.act2(X)\n",
    "        \n",
    "        X = self.hidden3(X)\n",
    "        X = self.act3(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xchnCuXQGrli"
   },
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bmyUZVM6Grlj"
   },
   "outputs": [],
   "source": [
    "def prepare_data(path):\n",
    "    dataset = CSVDataset(path)\n",
    "    train_data, test_data = dataset.get_split_data()\n",
    "    train_dl = DataLoader(train_data,batch_size = 32, shuffle = True)\n",
    "    test_dl = DataLoader(test_data, batch_size = 1024, shuffle = False)\n",
    "    \n",
    "    return train_dl, test_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HwFAhl15Grlm"
   },
   "source": [
    "### Train the model\n",
    "The training process requires that you define a loss function and an optimization algorithm.\n",
    "Common loss functions include the following:\n",
    "• BCELoss: Binary cross-entropy loss for binary classification.\n",
    "• CrossEntropyLoss: Categorical cross-entropy loss for multi-class classification.\n",
    "• MSELoss: Mean squared loss for regression.\n",
    "For more on loss functions generally, see the tutorial:\n",
    "• Loss and Loss Functions for Training Deep Learning Neural Networks\n",
    "Stochastic gradient descent is used for optimization, and the standard algorithm is provided by the SGD class, although other versions of the algorithm are available, such as Adam.\n",
    "\n",
    "Training the model involves enumerating the DataLoader for the training dataset.\n",
    "First, a loop is required for the number of training epochs. Then an inner loop is required for the mini-batches for stochastic gradient descent.\n",
    "\n",
    "Each update to the model involves the same general pattern comprised of:\n",
    "• Clearing the last error gradient.\n",
    "• A forward pass of the input through the model.\n",
    "• Calculating the loss for the model output.\n",
    "• Backpropagating the error through the model.\n",
    "• Update the model in an effort to reduce loss.\n",
    "\n",
    "The model is optimized using stochastic gradient descent and seeks to minimize the binary cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xk9ma3NzGrln"
   },
   "outputs": [],
   "source": [
    "def train_model(model,train_dl):\n",
    "    criterion = BCELoss()\n",
    "    optimizer = SGD(model.parameters(),lr = 0.01, momentum =0.9)\n",
    "    \n",
    "    for epoch in range(100):\n",
    "        for i,(inputs,target) in enumerate(train_dl):\n",
    "            optimizer.zero_grad()\n",
    "            yhat = model(inputs)\n",
    "            loss = criterion(yhat,target)\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8I1GwmJMGrlq"
   },
   "source": [
    "### Evaluate model\n",
    "Once the model is fit, it can be evaluated on the test dataset.\n",
    "This can be achieved by using the DataLoader for the test dataset and collecting the predictions for the test set, then comparing the predictions to the expected values of the test set and calculating a performance metric.\n",
    "\n",
    "accuracy_score(y_true, y_pred, normalize=True, sample_weight=None) --> The accuracy_score function computes the accuracy, either the fraction (default) or the count (normalize=False) of correct predictions.\n",
    "In multilabel classification, the function returns the subset accuracy. If the entire set of predicted labels for a sample strictly match with the true set of labels, then the subset accuracy is 1.0; otherwise it is 0.0.\n",
    "\n",
    "More details: https://scikit-learn.org/stable/modules/model_evaluation.html#accuracy-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FFl-_RHYGrlr"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model,test_dl):\n",
    "    prediction, actual = list(),list()\n",
    "    for i,(inputs,target) in enumerate(test_dl):\n",
    "        yhat = model(inputs)\n",
    "        \n",
    "        yhat = yhat.detach().numpy()\n",
    "        yhat = yhat.round()\n",
    "        \n",
    "        y = target.numpy()\n",
    "        y = y.reshape(len(y),1)\n",
    "        \n",
    "        prediction.append(yhat)\n",
    "        actual.append(y)\n",
    "    prediction, actual = vstack(prediction), vstack(actual)\n",
    "    \n",
    "    acc = accuracy_score(actual,prediction)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uSupXp_pGrlt"
   },
   "source": [
    "### Make predictions\n",
    "A fit model can be used to make a prediction on new data.\n",
    "For example, you might have a single image or a single row of data and want to make a prediction.\n",
    "This requires that you wrap the data in a PyTorch Tensor data structure.\n",
    "A Tensor is just the PyTorch version of a NumPy array for holding data. It also allows you to perform the automatic differentiation tasks in the model graph, like calling backward() when training the model.\n",
    "The prediction too will be a Tensor, although you can retrieve the NumPy array by detaching the Tensor from the automatic differentiation graph and calling the NumPy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z1Rsxn9rGrlu"
   },
   "outputs": [],
   "source": [
    "def predict(row,model):\n",
    "    row = Tensor([row])\n",
    "    yhat = model(row)\n",
    "    yhat = yhat.detach().numpy()\n",
    "    return yhat    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "Uy0FfiLmGrlw",
    "outputId": "099fc41a-f3f9-4a0a-dfd4-ad847ca00ba3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235 116\n",
      "Accuracy  0.9310344827586207\n",
      "Predicted: 0.981 (class=1)\n",
      "Predicted: 0.003 (class=0)\n"
     ]
    }
   ],
   "source": [
    "path = \"https://archive.ics.uci.edu/ml/machine-learning-databases/ionosphere/ionosphere.data\"\n",
    "train_dl, test_dl = prepare_data(path)\n",
    "print(len(train_dl.dataset), len(test_dl.dataset))\n",
    "\n",
    "model = MLP(34)\n",
    "\n",
    "train_model(model,train_dl)\n",
    "\n",
    "acc = evaluate_model(model,test_dl)\n",
    "print(\"Accuracy \", acc)\n",
    "\n",
    "row = [1,0,1,0.06655,1,-0.18388,1,-0.27320,1,-0.43107,1,-0.41349,0.96232,-0.51874,0.90711,-0.59017,0.89230,-0.66474,0.69876,-0.70997,0.70645,-0.76320,0.63081,-0.80544,0.55867,-0.89128,0.47211,-0.86500,0.40303,-0.83675,0.30996,-0.89093,0.22995,-0.89158]\n",
    "yhat = predict(row,model)\n",
    "print('Predicted: %.3f (class=%d)' % (yhat, yhat.round()))\n",
    "\n",
    "row = [1,0,0.71253,-0.02595,0.41287,-0.23067,0.98019,-0.09473,0.99709,-0.10236,1,-0.10951,0.58965,1,0.83726,-1,0.82270,-0.17863,0.80760,-0.28257,-0.25914,0.92730,0.51933,0.05456,0.65493,-0.20392,0.93124,-0.41307,0.63811,-0.21901,0.86136,-0.87354,-0.23186,-1]\n",
    "yhat = predict(row,model)\n",
    "print('Predicted: %.3f (class=%d)' % (yhat, yhat.round()))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "ionosphere.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
